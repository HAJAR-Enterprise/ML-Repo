{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPquiwbKSJg6"
      },
      "source": [
        "# Proyek NLP - [Classification of Online Gambling Comments]\n",
        "\n",
        "**Judul Proyek :** HAJAR (Hapus Judi Online Anti Ribet)\n",
        "\n",
        "**ID Team :** CC25-CF230\n",
        "\n",
        "**Anggota Team :**\n",
        "\n",
        "1. (ML) MC009D5Y0493 - Ahmad Zaky Humami – Universitas Gunadarma - Aktif\n",
        "2. (ML) MC009D5Y0506 - Fahru Rahman – Universitas Gunadarma - Aktif\n",
        "3. (ML) MC314D5X1177 - Shofi Shulhiyana – Universitas Singaperbangsa Karawang - Aktif\n",
        "4. (FEBE) FC009D5Y0885 - Muhammad Faris Rasyid Raharjo - Universitas Gunadarma  Aktif\n",
        "5. (FEBE) FC314D5Y1568 - Fahry Firdaus Marpaung - Universitas Singaperbangsa Karawang - Aktif\n",
        "6. (FEBE) FC009D5Y1828 - Linggar Riza Hamretta - Universitas Gunadarma - Aktif\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ohw73dMwbCBq"
      },
      "source": [
        "## Proyek Overview\n",
        "YouTube merupakan salah satu platform media sosial berbasis video terbesar di dunia yang memungkinkan penggunanya untuk dapat saling berinteraksi melalui kolom komentar. Namun, akhir-akhir ini kolom komentar pada konten video YouTube sering disalahgunakan oleh pihak yang tidak bertanggungjawab untuk menyebarkan konten spam dan promosi seputar judi online. Komentar tersebut mengganggu kenyamanan pengguna ketika ingin berdiskusi di kolom komentar dan berpotensi membahayakan jika terdapat tautan ke situs judi online tersebut. Pelaku spam juga kerap menggunakan kata-kata tersamar, dan simbol.\n",
        "\n",
        "Mengapa Masalah Ini Harus Diselesaikan :\n",
        "1.  **Mengganggu Kenyamanan Pengguna**: Kolom komentar yang disalahgunakan dapat mengganggu kenyamanan pengguna ketika ingin berdiskusi di kolom komentar. Hal ini dapat mengurangi pengalaman pengguna dan membuat mereka tidak ingin kembali ke platform tersebut.\n",
        "2.  **Bahaya Konten Judi Online**: Konten judi online dapat membahayakan pengguna, terutama anak-anak dan remaja yang masih belum dewasa. Mereka mungkin tidak memiliki pengetahuan yang cukup untuk mengenali bahaya konten tersebut dan dapat terjebak dalam situasi yang tidak diinginkan. Selain itu, konten judi online juga dapat membahayakan pengguna yang sudah dewasa karena dapat menyebabkan kecanduan dan masalah keuangan. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90P19MERbNBV"
      },
      "source": [
        "## Business Understanding\n",
        "\n",
        "### Problem Statements\n",
        "1. \n",
        "2. \n",
        "3. \n",
        "\n",
        "### Goals\n",
        "1. \n",
        "2. \n",
        "3. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCkr3Fxca5TN"
      },
      "source": [
        "## Data Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px-4J_4E4eZW"
      },
      "source": [
        "### Import Library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "DAr2qaJ8kjSe",
        "outputId": "4cb5d67a-6050-4b62-c34b-20237f067c22"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Text preprocessing\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from ftfy import fix_text\n",
        "\n",
        "# Train/test split & metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Deep learning with TensorFlow/Keras\n",
        "import tensorflow as tf\n",
        "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
        "from collections import Counter\n",
        "\n",
        "# Classical ML models\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Word2Vec embeddings\n",
        "from wordcloud import WordCloud\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ytf64WS7aNZe"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt_tab', download_dir='C:/nltk_data')\n",
        "nltk.download('punkt', download_dir='C:/nltk_data')\n",
        "nltk.download('stopwords', download_dir='C:/nltk_data')\n",
        "nltk.data.path.append('C:/nltk_data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBvPl_VJ4heA"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OCxTv2ta0QG"
      },
      "source": [
        "Dataset For Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oojz6uAakvk7"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv('Datasets/youtube_comments.csv')\n",
        "df2 = pd.read_csv('Datasets/komentar_judi.csv')\n",
        "df3 = pd.read_csv('Datasets/komentar_judi2.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rErnbTMGcH9E"
      },
      "source": [
        "### Deskripsi Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1q9vtWVCcObY"
      },
      "source": [
        "Variabel | Keterangan\n",
        "----------|----------\n",
        "Author | Unique username.\n",
        "Comment  | user comments from YouTube videos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JydxSH9TidWA"
      },
      "outputs": [],
      "source": [
        "df1.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXw7uJ7VEDUH"
      },
      "outputs": [],
      "source": [
        "df1.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pGYFDNnEIeO"
      },
      "outputs": [],
      "source": [
        "print(\"Duplicated : \", df1.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-ZQYWD7ijDT"
      },
      "outputs": [],
      "source": [
        "df2.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFKaUEHEEAJC"
      },
      "outputs": [],
      "source": [
        "df2.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8nUHriiEM9k"
      },
      "outputs": [],
      "source": [
        "print(\"Duplicated : \", df2.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZymWnyOihpy"
      },
      "outputs": [],
      "source": [
        "df3.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uQGkPl_D8Om"
      },
      "outputs": [],
      "source": [
        "df3.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PeYzgD-EfOD"
      },
      "outputs": [],
      "source": [
        "print(\"Duplicated : \", df3.duplicated().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VYFDdAuDcex"
      },
      "source": [
        "#### Handling Missing Values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imiRQMv9Dgcc"
      },
      "outputs": [],
      "source": [
        "df1 = df1.dropna()\n",
        "\n",
        "print(\"Missing values after dropping NaNs:\")\n",
        "print(\"Dataset 1 missing values:\\n\", df1.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiohYZW6Dpbw"
      },
      "outputs": [],
      "source": [
        "df2 = df2.dropna()\n",
        "\n",
        "print(\"Missing values after dropping NaNs:\")\n",
        "print(\"Dataset 2 missing values:\\n\", df2.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEEY1g82DseX"
      },
      "outputs": [],
      "source": [
        "df3 = df3.dropna()\n",
        "\n",
        "print(\"Missing values after dropping NaNs:\")\n",
        "print(\"Dataset 3 missing values:\\n\", df3.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHObJGPW4o_A"
      },
      "source": [
        "### Merge Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6lkkHZIzCj5"
      },
      "outputs": [],
      "source": [
        "df_combined = pd.concat([df1, df2, df3], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2yrhnWRiYd-"
      },
      "outputs": [],
      "source": [
        "df_combined.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rzm8i73iS3_"
      },
      "outputs": [],
      "source": [
        "df_combined.head(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvSTIedMjqYt"
      },
      "source": [
        "Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbX8La60jsO1"
      },
      "outputs": [],
      "source": [
        "df_combined.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6pRfmVT4z6w"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SPIWMceux33"
      },
      "outputs": [],
      "source": [
        "# Drop a one word comment\n",
        "df_combined = df_combined[df_combined['comment'].str.split().str.len() > 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft6SdzzHi_tb"
      },
      "source": [
        "Hapus missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4dxSYsyvBWG"
      },
      "outputs": [],
      "source": [
        "# Remove rows with any missing values\n",
        "df_combined.dropna(inplace=True)\n",
        "\n",
        "print(\"Missing values per column: \\n\")\n",
        "print(df_combined.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlS9pi5O45lS"
      },
      "source": [
        "### Cleaning text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Converting all the characters in a text into lower case\n",
        "def casefoldingText(text):\n",
        "      return text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_unicode_to_ascii(text):\n",
        "      \"\"\"Normalize text to ASCII, preserving spaces\"\"\"\n",
        "      text = fix_text(text)\n",
        "\n",
        "      if isinstance(text, str):\n",
        "            # Normalize Unicode to decomposed form\n",
        "            text = unicodedata.normalize('NFKD', text)\n",
        "            # Ganti karakter non-ASCII dengan spasi daripada menghapusnya\n",
        "            text = ''.join(ch if ord(ch) < 128 else ' ' for ch in text)\n",
        "\n",
        "      return text\n",
        "\n",
        "print(normalize_unicode_to_ascii(\"gachoг m𝘶lu ԁi 𝘿𝙊 𝙍 𝘼 𝟳 𝟳🙍!\"))\n",
        "print(normalize_unicode_to_ascii(\"Pngguna bru 🛑𝐊𝗨𝐒𝗨𝐌𝗔𝐓𝟬𝐓𝟬🚦,pm aja\"))\n",
        "print(normalize_unicode_to_ascii(\"Gaji numpang lewat? Biarin, ada ♛𝗔𝗦𝗜𝗔𝗚𝗘𝗡𝗧𝗜𝗡𝗚♛\"))\n",
        "print(normalize_unicode_to_ascii(\"😎: Betul Bro ⚡𝗦𝗨𝗣𝗘𝗥𝗠𝗢𝗡𝗘𝗬𝟴𝟴⚡⚡𝗦𝗨𝗣𝗘𝗥𝗠𝗢𝗡𝗘𝗬𝟴𝟴⚡\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bdt-ZQl4Cnjp"
      },
      "outputs": [],
      "source": [
        "# Function untuk Cleaning Text\n",
        "def cleaningText(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Remove mentions (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    # Remove hashtags (#hashtag)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    \n",
        "    # Replace comma in the middle of the text with space\n",
        "    text = re.sub(r',', ' ', text)\n",
        "\n",
        "    # Remove the numbers, but keep the numbers attached to the word\n",
        "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
        "    \n",
        "    # Remove non-alphanumeric characters at the beginning or end of the string\n",
        "    text = re.sub(r'^[^a-zA-Z0-9]+|[^a-zA-Z0-9]+$', ' ', text)\n",
        "\n",
        "    text = text.translate(str.maketrans(' ', ' ', string.punctuation)) # remove all punctuations\n",
        "    \n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)# replace new line into space\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # remove characters space from both left and right text\n",
        "    \n",
        "    text = re.sub(r'(\\b[a-z]+) (\\d+\\b)', r'\\1\\2', text)\n",
        "    \n",
        "    \n",
        "\n",
        "    return text\n",
        "\n",
        "text = \"gachoг m𝘶lu ԁi 𝘿𝙊 𝙍 𝘼 𝟳 𝟳🙍!\"\n",
        "text = normalize_unicode_to_ascii(text)\n",
        "print(text)\n",
        "print(cleaningText(text))\n",
        "\n",
        "print(cleaningText(normalize_unicode_to_ascii(\"gachoг m𝘶lu ԁi 𝘿𝙊 𝙍 𝘼 𝟳 𝟳🙍!\")))\n",
        "print(cleaningText(normalize_unicode_to_ascii(\"Pngguna bru 🛑𝐊𝗨𝐒𝗨𝐌𝗔𝐓𝟬𝐓𝟬🚦,pm aja\")))\n",
        "print(cleaningText(normalize_unicode_to_ascii(\"Gaji numpang lewat? Biarin, ada ♛𝗔𝗦𝗜𝗔𝗚𝗘𝗡𝗧𝗜𝗡𝗚♛\")))\n",
        "print(cleaningText(normalize_unicode_to_ascii(\"😎: Betul Bro ⚡𝗦𝗨𝗣𝗘𝗥𝗠𝗢𝗡𝗘𝗬𝟴𝟴⚡⚡𝗦𝗨𝗣𝗘𝗥𝗠𝗢𝗡𝗘𝗬𝟴𝟴⚡\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rZM9mTyOz7u"
      },
      "source": [
        "### Tokenizing text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CECJRQEZOzkX"
      },
      "outputs": [],
      "source": [
        "def tokenizingText(text): # Tokenizing or splitting a string, text into a list of tokens\n",
        "    text = word_tokenize(text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myWDkISMTvrM"
      },
      "source": [
        "### Removing Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIaZYD3wT1fM"
      },
      "outputs": [],
      "source": [
        "def filteringText(text): # Remove stopwors in a text\n",
        "    listStopwords = set(stopwords.words('indonesian'))\n",
        "    listStopwords1 = set(stopwords.words('english'))\n",
        "    listStopwords.update(listStopwords1)\n",
        "    listStopwords.update(['iya','yaa','gak','nya','na','sih','ku',\"di\",\"ga\",\"ya\",\"gaa\",\"loh\",\"kah\",\"woi\",\"woii\",\"woy\"])\n",
        "    filtered = []\n",
        "    for txt in text:\n",
        "        if txt not in listStopwords:\n",
        "            filtered.append(txt)\n",
        "    text = filtered\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn_G7kmvUFeZ"
      },
      "source": [
        "### Stemming text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ed5q1TQUHxc"
      },
      "outputs": [],
      "source": [
        "def stemmingText(text): # Reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words\n",
        "    # Membuat objek stemmer\n",
        "    factory = StemmerFactory()\n",
        "    stemmer = factory.create_stemmer()\n",
        "\n",
        "    # Memecah teks menjadi daftar kata\n",
        "    words = text.split()\n",
        "\n",
        "    # Menerapkan stemming pada setiap kata dalam daftar\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    # Menggabungkan kata-kata yang telah distem\n",
        "    stemmed_text = ' '.join(stemmed_words)\n",
        "\n",
        "    return stemmed_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QJalx7dUaP6"
      },
      "source": [
        "### Convert to sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1LxpbJlUd8p"
      },
      "outputs": [],
      "source": [
        "def toSentence(list_words): # Convert list of words into sentence\n",
        "    sentence = ' '.join(word for word in list_words)\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2__RdU8uVJFR"
      },
      "source": [
        "### Correcting the slang words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bdo1wn3i7o93"
      },
      "outputs": [],
      "source": [
        "def fix_slangwords(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    slang_dict = {\n",
        "        # Negasi & Penyangkalan\n",
        "        \"gk\": \"tidak\", \"gak\": \"tidak\", \"ga\": \"tidak\", \"g\": \"tidak\", \"nggak\": \"tidak\", \"enggak\": \"tidak\",\n",
        "        \"gpp\": \"tidak apa-apa\", \"gakpapa\": \"tidak apa-apa\", \"gabut\": \"tidak ada kerjaan\",\n",
        "        \"gapapa\": \"tidak apa-apa\", \"gapapa\": \"tidak apa-apa\", \"ngga\": \"tidak\", \"nggak\": \"tidak\",\n",
        "        \"nggakpapa\": \"tidak apa-apa\", \"nggpp\": \"tidak apa-apa\", \"nggaapa\": \"tidak apa-apa\",\n",
        "\n",
        "        # Penghubung & Penjelas\n",
        "        \"tp\": \"tetapi\", \"tapi\": \"tetapi\", \"kl\": \"kalau\", \"klw\": \"kalau\", \"kalo\": \"kalau\", \"krn\": \"karena\",\n",
        "        \"karena\": \"karena\", \"jd\": \"jadi\", \"jg\": \"juga\", \"aja\": \"saja\", \"sih\": \"\", \"kok\": \"mengapa\",\n",
        "        \"dl\": \"dulu\", \"pdhl\": \"padahal\", \"btw\": \"ngomong-ngomong\", \"spt\": \"seperti\",\n",
        "\n",
        "        # Kata ganti orang\n",
        "        \"sy\": \"saya\", \"gw\": \"saya\", \"gue\": \"saya\", \"gua\": \"saya\", \"w\": \"saya\", \"gwe\": \"saya\",\n",
        "        \"q\": \"aku\", \"ak\": \"aku\", \"aq\": \"aku\", \"km\": \"kamu\", \"lu\": \"kamu\", \"lo\": \"kamu\", \"elo\": \"kamu\",\n",
        "        \"elu\": \"kamu\", \"loe\": \"kamu\", \"u\": \"kamu\", \"i\": \"saya\", \"tmn\": \"teman\", \"tmn2\": \"teman-teman\",\n",
        "\n",
        "        # Kata kerja / tindakan\n",
        "        \"udh\": \"sudah\", \"udah\": \"sudah\", \"sdh\": \"sudah\", \"lg\": \"lagi\", \"bikin\": \"membuat\",\n",
        "        \"ksih\": \"kasih\", \"ksh\": \"kasih\", \"jgn\": \"jangan\", \"jangan\": \"jangan\", \"biar\": \"agar\",\n",
        "        \"supaya\": \"agar\", \"bisa\": \"bisa\", \"bs\": \"bisa\", \"bsa\": \"bisa\", \"sabi\": \"bisa\", \"dlm\": \"dalam\",\n",
        "        \"belain\": \"membela\", \"belainin\": \"membela\", \"bela\": \"membela\", \"bales\": \"membalas\",\n",
        "        \"balas\": \"membalas\", \"balikin\": \"mengembalikan\", \"balikinya\": \"mengembalikannya\",\n",
        "        \"balikinya\": \"mengembalikannya\", \"balikin aja\": \"mengembalikannya\", \"balikin dong\": \"mengembalikannya\",\n",
        "\n",
        "        # Kata benda / objek\n",
        "        \"org\": \"orang\", \"modal\": \"uang\", \"cuan\": \"untung\", \"bonus\": \"hadiah\", \"jp\": \"jackpot\",\n",
        "        \"jepe\": \"jackpot\", \"jepey\": \"jackpot\", \"slot\": \"permainan judi\", \"betting\": \"taruhan\",\n",
        "        \"promo\": \"promosi\", \"event\": \"acara\", \"depo\": \"deposit\", \"wd\": \"withdraw\",\n",
        "\n",
        "        # Emosi dan ekspresi informal\n",
        "        \"anjay\": \"astaga\", \"anjir\": \"astaga\", \"anjrit\": \"astaga\", \"wkwk\": \"haha\", \"wkwkwk\": \"haha\",\n",
        "        \"wk\": \"haha\", \"lol\": \"haha\", \"ngakak\": \"tertawa\", \"baper\": \"terbawa perasaan\",\n",
        "        \"kepo\": \"penasaran\", \"julid\": \"iri\", \"gibah\": \"bergosip\", \"santuy\": \"santai\", \"woles\": \"santai\",\n",
        "        \"mager\": \"malas\", \"lebay\": \"berlebihan\", \"pecah\": \"seru\", \"ngablu\": \"mengigau\", \"cape\": \"capek\",\n",
        "        \"capekkk\": \"capek\", \"pusinggg\": \"pusing\", \"ngeri\": \"hebat\", \"goks\": \"hebat\", \"receh\": \"tidak penting\",\n",
        "        \"mantul\": \"bagus\", \"mantab\": \"mantap\", \"uhuy\": \"mantap\", \"skuy\": \"ayo\", \"gas\": \"ayo\",\n",
        "        \"gaskeun\": \"ayo\", \"panik\": \"takut\", \"bgt\": \"banget\", \"banget\": \"sekali\", \"auto\": \"langsung\",\n",
        "        \"halu\": \"berkhayal\", \"sabi\": \"bisa\",\n",
        "\n",
        "        # Kata rujukan/julukan\n",
        "        \"min\": \"admin\", \"bang\": \"kakak\", \"bg\": \"kakak\", \"bng\": \"kakak\", \"kak\": \"kakak\", \"bro\": \"saudara\",\n",
        "        \"sis\": \"kakak\", \"ngab\": \"teman\", \"cuy\": \"teman\", \"ngabers\": \"remaja pria\", \"mrk\": \"mereka\",\n",
        "        \"sm\": \"sama\", \"sama\": \"dengan\", \"dg\": \"dengan\", \"dr\": \"dari\", \"utk\": \"untuk\", \"yg\": \"yang\",\n",
        "        \"dll\": \"dan lain-lain\", \"dst\": \"dan seterusnya\", \"ttp\": \"tetap\", \"tsb\": \"tersebut\",\n",
        "        \"mnrt\": \"menurut\", \"jdwal\": \"jadwal\", \"bener\": \"benar\", \"d\": \"di\", \"emg\": \"memang\", \"emng\": \"memang\",\n",
        "        \"bocil\": \"anak kecil\", \"gacr\": \"gacor\", \"gacir\": \"gacor\", \"gcr\": \"gacor\", \"mekswin\": \"maxwin\",\n",
        "        \"win\": \"menang\", \"gmpng\": \"mudah\", \"gampang\": \"mudah\", \"bet\": \"banget\", \"nasib\": \"keberuntungan\"\n",
        "    }\n",
        "\n",
        "    words = text.split()\n",
        "    new_words = [slang_dict.get(word.lower(), word) for word in words]\n",
        "    return ' '.join(new_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXYodPuFXQ4Y"
      },
      "outputs": [],
      "source": [
        "# Applies a series of text preprocessing functions to the 'comment' column of the DataFrame.\n",
        "df_combined.loc[:,'normalizeText'] = df_combined['comment'].apply(normalize_unicode_to_ascii)\n",
        "df_combined['cleanText'] = df_combined['normalizeText'].apply(cleaningText)\n",
        "df_combined['casefoldingText'] = df_combined['cleanText'].apply(casefoldingText)\n",
        "df_combined['fixSlangWords'] = df_combined['casefoldingText'].apply(fix_slangwords)\n",
        "df_combined['stemmingText'] = df_combined['fixSlangWords'].apply(stemmingText)\n",
        "df_combined['tokenizingText'] = df_combined['stemmingText'].apply(tokenizingText)\n",
        "df_combined['stopWordText'] = df_combined['tokenizingText'].apply(filteringText)\n",
        "df_combined['finalText'] = df_combined['stopWordText'].apply(toSentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sukVvtjiTbAy"
      },
      "outputs": [],
      "source": [
        "df_combined.head(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81tWMfSLW3DP"
      },
      "source": [
        "### Encode Labeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Label Judol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jl8jNXitwUo5"
      },
      "outputs": [],
      "source": [
        "# Define keywords for gambling comments\n",
        "gambling_keywords = [\n",
        "    'gacor', 'gacor mulu', 'g4cor', 'g4cor mulu', 'g4c0r', 'g4c0r mulu', 'gac0r', 'gac0r mulu', 'gachor', 'gachor mulu', 'gachog', 'gachog mulu', 'gacho', 'gacho mulu',\n",
        "    'jepe', 'jepe terus', 'j3pe', 'j3pe terus', 'jep3', 'jep3 terus', 'j3p3', 'j3p3 terus', 'jp', 'jp terus', 'jackpot', 'jackpot terus', 'jekpot', 'jekpot terus', 'j3kpot', 'j3kpot terus', 'j3kp0t', 'j3kp0t terus', 'jekp0t', 'jekp0t terus', 'j4ckp0t', 'j4ckp0t terus', 'j4ckpot', 'j4ckpot terus',\n",
        "    'jackpot mulu', 'jackpot terus', 'jackpot hari ini', 'jackpot mudah menang', 'jackpot gampang menang',\n",
        "    'jackpot maxwin', 'jackpot gacor', 'jackpot gacor hari ini', 'jackpot gacor terbaru', 'jackpot gacor maxwin',\n",
        "    'jackpot gacor mudah menang', 'jackpot slot', 'jackpot judi', 'jackpot online', 'jackpot slot online',\n",
        "    'bonus', 'b0nus', 'klaim', 'hoki', 'h0k1', 'hok1', 'h0ki', 'cuan', 'cu4n', 'menang', 'm3nang', 'menang terus', 'm3ang terus',\n",
        "    'main', 'main slot', 'main judi', 'judi online', 'judi slot', 'judi slot online', 'judi slot gacor',\n",
        "    'judi slot terbaru', 'judi slot hari ini', 'judi slot gampang menang', 'judi slot mudah menang', 'judi slot maxwin',\n",
        "    'judi slot gacor hari ini', 'judi slot gacor terbaru', 'judi slot gacor maxwin', 'judi slot gacor mudah menang',\n",
        "    'spin', 'free spin', 'auto win', 'pola', 'wd', 'withdraw', 'depo', 'deposit', 'withdrawal', 'saldo',\n",
        "    'deposit pulsa', 'deposit ovo', 'deposit dana', 'deposit gopay', 'deposit via pulsa', 'deposit via ovo',\n",
        "    'deposit via dana', 'deposit via gopay', 'withdraw pulsa', 'withdraw ovo', 'withdraw dana', 'withdraw gopay',\n",
        "    'withdraw via pulsa', 'withdraw via ovo', 'withdraw via dana', 'withdraw via gopay', 'deposit bank',\n",
        "    'withdraw bank', 'deposit bank lokal', 'withdraw bank lokal', 'deposit bank online', 'withdraw bank online',\n",
        "    'bandar', 'situs', 'toto', 'togel', 'sl0t', 'slot', 'slot online', 'game slot', 'link slot', 'link gacor',\n",
        "    'link alternatif', 'link judi', 'link slot gacor', 'link slot terbaru', 'link slot hari ini', 'link slot mudah menang',\n",
        "    'pr0be855', 'weton88', 'pulauwin', '25kbet', 'alexis17', 'alexis', 'berkah99', 'aero88', 'sgi88', 'pluto88',\n",
        "    'sultan88', 'sultanbet', 'sultanbet88', 'sultanbet99', 'sultanbet77', 'sultanbet88', 'sultanbet99', 'sultanbet77',\n",
        "    'garudahoki', 'mona4d', 'berlian', 'btv', 'xuxu4d', 'pstoto99', 'daftar sekarang', 'join sekarang', 'link alternatif',\n",
        "    'login disini', 'klik disini', 'event harian', 'event mingguan', 'turnover', 'rollingan', 'komisi', 'claim sekarang',\n",
        "    'claim bonus', 'claim hadiah', 'claim jackpot', 'claim jepe', 'claim jp', 'claim bonus harian', 'claim bonus mingguan',\n",
        "    'claim bonus bulanan', 'claim bonus tahunan', 'claim bonus slot', 'claim bonus judi', 'claim bonus gacor',\n",
        "    'live casino', 'judi', 'casino', 'tembus', 'untung terus', 'deposit via dana', 'via gopay', 'via ovo', 'via pulsa',\n",
        "    'slot terpercaya', 'slot terbaru', 'promo deposit', 'promosi slot', 'event slot', 'winrate tinggi', 'maxwin', 'm4xw1n',\n",
        "    'maxwin mulu', 'maxwin terus', 'maxwin hari ini', 'maxwin slot', 'maxwin judi', 'maxwin gacor', 'maxwin mudah menang',\n",
        "    'pr0m0', 'promo', 'link alternatif', 'slot maxwin', 'slot pragmatic', 'slot demo', 'slot terbaru hari ini', 'asiagenting',\n",
        "    'slot tergacor', 'slot terbaik', 'bet', 'betting', 'big win', 'winrate', 'modal receh', 'main disini', 'langsung gas',\n",
        "    'langsung menang', 'langsung jackpot', 'langsung jepe', 'langsung jp', 'langsung gacor', 'langsung auto win',\n",
        "    'spin gratis', 'rtp tinggi', 'rtp slot', 'slot mudah menang', 'slot hari ini', 'jp terus', 'win terus', 'situs terpercaya',\n",
        "    'situs judi', 'situs slot', 'situs slot online', 'situs judi online', 'situs slot gacor', 'situs slot terbaru',\n",
        "    'situs slot hari ini', 'situs slot mudah menang', 'situs slot maxwin', 'situs judi terpercaya', 'situs judi online terpercaya',\n",
        "    'slot online terpercaya', 'gunungwin', 'ayamwin', 'pulau777', 'pulau7', 'zeus', 'kusumat0t0', 'pecahan', 'maxwin', 'supermoney88',\n",
        "    'supermoney', 'supermoney88supermoney88', 'supermoney77', 'supermoney99',\n",
        "    'dora', 'd ora', 'do ra', 'dor a', 'd o ra', 'd or a', 'do r a', 'd o r a', 'dora77', 'ora77', 'a77', ' 77',' 7 7 ',\n",
        "    'probe855', 'probe 855', 'pro be 855', 'pro be855', 'pro be 8 5 5', 'pr0be855', 'pr0be 855', 'pr0 be 855', 'pr0 be855', 'pr0 be 8 5 5', 'probe', 'pr0be',\n",
        "]\n",
        "\n",
        "# Function to check if a comment contains any gambling keyword\n",
        "def is_gambling_comment(comment):\n",
        "    if isinstance(comment, str):\n",
        "        # Ensure the comment is lowercase and split into words for accurate matching\n",
        "        words = comment.lower().split()\n",
        "        for keyword in gambling_keywords:\n",
        "            # Check if the keyword exists as a whole word in the comment\n",
        "            if keyword in words:\n",
        "                return 1 # Label 1 for gambling\n",
        "        return 0 # Label 0 for not gambling\n",
        "    return 0 # Default to 0 if comment is not a string\n",
        "\n",
        "# Apply the labeling function to the 'cleaned_comment' column\n",
        "df_combined['label'] = df_combined['finalText'].apply(is_gambling_comment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xm3kqQrebxxO"
      },
      "outputs": [],
      "source": [
        "df_combined[df_combined['label'] == 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mhn-BftvbcKB"
      },
      "outputs": [],
      "source": [
        "df_combined[df_combined['label'] == 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcmP7JUy7biD"
      },
      "outputs": [],
      "source": [
        "label_counts = df_combined['label'].value_counts()\n",
        "\n",
        "# Labels for the pie chart\n",
        "labels = ['Not Gambling (0)', 'Gambling (1)']\n",
        "sizes = label_counts.values\n",
        "colors = ['lightblue', 'lightcoral']\n",
        "explode = (0.1, 0)\n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n",
        "plt.title('Distribution of Comment Labels')\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.legend(title=\"Label\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Label Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lexicon_positive = dict()\n",
        "lexicon_negative = dict()\n",
        "\n",
        "response_positive = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_positive.csv')\n",
        "if response_positive.status_code == 200:\n",
        "      reader = csv.reader(StringIO(response_positive.text), delimiter=',')\n",
        "      for row in reader:\n",
        "            lexicon_positive[row[0]] = int(row[1])\n",
        "else:\n",
        "      print(\"Failed to fetch positive lexicon data\")\n",
        "\n",
        "response_negative = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_negative.csv')\n",
        "if response_negative.status_code == 200:\n",
        "      reader = csv.reader(StringIO(response_negative.text), delimiter=',')\n",
        "      for row in reader:\n",
        "            lexicon_negative[row[0]] = int(row[1])\n",
        "else:\n",
        "      print(\"Failed to fetch negative lexicon data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sentiment_analysis_lexicon_indonesia(text):\n",
        "      score = 0\n",
        "      \n",
        "      for word in text:\n",
        "            if word in lexicon_positive:\n",
        "                  score += lexicon_positive[word]\n",
        "            \n",
        "            elif word in lexicon_negative:\n",
        "                  score += lexicon_negative[word]\n",
        "      \n",
        "      if score >= 0:\n",
        "            polarity = 'positive'\n",
        "      else:\n",
        "            polarity = 'negative'\n",
        "\n",
        "      return score, polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = df_combined['stopWordText'].apply(sentiment_analysis_lexicon_indonesia)\n",
        "results = list(zip(*results))\n",
        "df_combined['polarity_score'] = results[0]\n",
        "df_combined['polarity'] = results[1]\n",
        "print(df_combined['polarity'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "sizes = [count for count in df_combined['polarity'].value_counts()]\n",
        "labels = list(df_combined['polarity'].value_counts().index)\n",
        "\n",
        "explode = (0.1, 0)\n",
        "ax.pie(x=sizes, labels=labels, autopct='%1.1f%%', explode=explode, textprops={'fontsize': 14})\n",
        "ax.set_title('Sentiment Polarity on Review Data', fontsize=16, pad=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', 2000)\n",
        "positive_tweets = df_combined[df_combined['polarity'] == 'positive']\n",
        "positive_tweets = positive_tweets[['finalText', 'polarity_score', 'polarity','stopWordText']]\n",
        "positive_tweets = positive_tweets.sort_values(by='polarity_score', ascending=False)\n",
        "positive_tweets = positive_tweets.reset_index(drop=True)\n",
        "positive_tweets.index += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', 2000)\n",
        "negative_tweets = df_combined[df_combined['polarity'] == 'negative']\n",
        "negative_tweets = negative_tweets[['finalText', 'polarity_score', 'polarity','stopWordText']]\n",
        "negative_tweets = negative_tweets.sort_values(by='polarity_score', ascending=True)\n",
        "negative_tweets = negative_tweets[0:10]\n",
        "negative_tweets = negative_tweets.reset_index(drop=True)\n",
        "negative_tweets.index += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list_words = ''\n",
        "for tweet in df_combined['stopWordText']:\n",
        "      for word in tweet:\n",
        "            list_words += ' ' + (word)\n",
        "\n",
        "wordcloud = WordCloud(width=600, height=400, background_color='white', min_font_size=10).generate(list_words)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.set_title('Awan Kata dari Seluruh Tweets ', fontsize=18)\n",
        "ax.grid(False)\n",
        "ax.imshow((wordcloud))\n",
        "fig.tight_layout(pad=0)\n",
        "ax.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Membuat string kosong 'list_words' yang akan digunakan untuk mengumpulkan semua kata dari teks yang sudah dibersihkan dalam tweet negatif.\n",
        "list_words = ''\n",
        "for tweet in negative_tweets['stopWordText']:\n",
        "      for word in tweet:\n",
        "            list_words += ' ' + (word)\n",
        "\n",
        "wordcloud = WordCloud(width=600, height=400, background_color='white', min_font_size=10).generate(list_words)\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.set_title('Awan Kata dari Tweets Negatif', fontsize=18)\n",
        "ax.grid(False)\n",
        "ax.imshow((wordcloud))\n",
        "fig.tight_layout(pad=0)\n",
        "ax.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Membuat string kosong 'list_words' yang akan digunakan untuk mengumpulkan semua kata dari teks yang sudah dibersihkan dalam tweet positif.\n",
        "list_words = ''\n",
        "for tweet in positive_tweets['stopWordText']:\n",
        "      for word in tweet:\n",
        "            list_words += ' ' + (word)\n",
        "\n",
        "wordcloud = WordCloud(width=600, height=400, background_color='white', min_font_size=10).generate(list_words)\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.set_title('Awan Kata dari Tweet Positif', fontsize=18)\n",
        "ax.grid(False)\n",
        "ax.imshow((wordcloud))\n",
        "fig.tight_layout(pad=0)\n",
        "ax.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Judol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_combined.head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(texts, labels, tokenizer):\n",
        "      \"\"\"Custom preprocessing untuk BERT-like model\"\"\"\n",
        "      print(f\"Preprocessing {len(texts)} texts...\")\n",
        "\n",
        "      clean_texts = []\n",
        "      clean_labels = []\n",
        "\n",
        "      for i, (text, label) in enumerate(zip(texts, labels)):\n",
        "            if isinstance(text, str) and len(text.strip()) > 0:\n",
        "                  clean_texts.append(text.strip())\n",
        "                  clean_labels.append(int(label))\n",
        "            else:\n",
        "                  print(f\"Skipping invalid text at index {i}: {text}\")\n",
        "\n",
        "      print(f\"Valid texts after cleaning: {len(clean_texts)}\")\n",
        "\n",
        "      # Encode menggunakan custom tokenizer\n",
        "      encoded = tokenizer(\n",
        "            clean_texts,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=128,\n",
        "            return_tensors=\"tf\"\n",
        "      )\n",
        "\n",
        "      return {\n",
        "            'input_ids': tf.constant(encoded['input_ids'], dtype=tf.int32),\n",
        "            'attention_mask': tf.constant(encoded['attention_mask'], dtype=tf.int32),\n",
        "            'labels': tf.constant(clean_labels, dtype=tf.int32)\n",
        "      }\n",
        "\n",
        "def create_dataset(data, batch_size=16, shuffle=True):\n",
        "      \"\"\"Create TensorFlow dataset in correct format for model\"\"\"\n",
        "      dataset = tf.data.Dataset.from_tensor_slices((\n",
        "            {\n",
        "                  'input_ids': data['input_ids'],\n",
        "                  'attention_mask': data['attention_mask']\n",
        "            },\n",
        "            data['labels']\n",
        "      ))\n",
        "\n",
        "      if shuffle:\n",
        "            dataset = dataset.shuffle(buffer_size=1000)\n",
        "\n",
        "      return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"Preprocessing functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Muat dataset\n",
        "texts = df_combined['finalText'].tolist()\n",
        "labels = df_combined['label'].tolist()\n",
        "\n",
        "print(f\"Dataset loaded: {len(texts)} samples\")\n",
        "print(f\"Label distribution: {Counter(labels)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split dataset\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "      texts, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_texts)}\")\n",
        "print(f\"Validation samples: {len(val_texts)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Creating custom BERT tokenizer...\")\n",
        "# tokenizer = CustomBERTTokenizer(vocab_size=30000, max_length=128)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
        "\n",
        "# Build vocabulary dari training data\n",
        "# tokenizer.build_vocab(train_texts)\n",
        "\n",
        "print(\"Tokenizer ready!\")\n",
        "\n",
        "# Test tokenizer\n",
        "sample_text = train_texts[0]\n",
        "encoded_sample = tokenizer.encode(sample_text)\n",
        "print(f\"\\nSample text: {sample_text}\")\n",
        "print(f\"Encoded input_ids: {encoded_sample[:10]}...\")\n",
        "print(f\"Attention mask: {encoded_sample[:10]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess training data\n",
        "print(\"Preprocessing training data...\")\n",
        "train_data = tokenize(train_texts, train_labels, tokenizer)\n",
        "\n",
        "# Preprocess validation data\n",
        "print(\"\\nPreprocessing validation data...\")\n",
        "val_data = tokenize(val_texts, val_labels, tokenizer)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = create_dataset(train_data, batch_size=16, shuffle=True)\n",
        "val_dataset = create_dataset(val_data, batch_size=16, shuffle=False)\n",
        "\n",
        "print(\"\\nData preprocessing completed!\")\n",
        "print(f\"Training data shape: {train_data['input_ids'].shape}\")\n",
        "print(f\"Validation data shape: {val_data['input_ids'].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IndoBERT model\n",
        "print(\"IndoBERT model...\")\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"indobenchmark/indobert-base-p1\", num_labels=2)\n",
        "\n",
        "# Compile model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "      metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"Model created and compiled successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup callbacks\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath='./custom_bert_model.keras',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.TensorBoard(\n",
        "        log_dir='./logs'\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"Training callbacks setup completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for batch in train_dataset.take(1):\n",
        "      print(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'train_dataset' in locals() and 'val_dataset' in locals():\n",
        "      # CELL 12: Train Model\n",
        "      # Start training\n",
        "      print(\"Starting training...\")\n",
        "      print(\"This may take a while depending on your dataset size...\")\n",
        "\n",
        "      history = model.fit(\n",
        "            train_dataset,\n",
        "            validation_data=val_dataset,\n",
        "            epochs=20,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "      )\n",
        "\n",
        "      print(\"Training completed!\")\n",
        "else:\n",
        "      print(\"Skipping model training due to empty dataset(s).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Evaluating model...\")\n",
        "\n",
        "val_predictions = []\n",
        "val_true_labels = []\n",
        "\n",
        "for batch in val_dataset:\n",
        "      inputs, labels = batch  # Unpack the inputs and labels\n",
        "      preds = model(inputs)\n",
        "\n",
        "      val_predictions.extend(tf.argmax(preds, axis=1).numpy())\n",
        "      val_true_labels.extend(labels.numpy())\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(val_true_labels, val_predictions)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(val_true_labels, val_predictions, average='weighted')\n",
        "\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Validation Precision: {precision:.4f}\")\n",
        "print(f\"Validation Recall: {recall:.4f}\")\n",
        "print(f\"Validation F1-score: {f1:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(val_true_labels, val_predictions, target_names=['Bukan Judi Online', 'Judi Online']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 14: Prediction Functions\n",
        "def predict_custom_bert(text, model, tokenizer):\n",
        "      \"\"\"Prediksi single text dengan custom BERT\"\"\"\n",
        "      encoded = tokenizer.encode(text)\n",
        "      inputs = {\n",
        "            'input_ids': tf.constant([encoded['input_ids']], dtype=tf.int32),\n",
        "            'attention_mask': tf.constant([encoded['attention_mask']], dtype=tf.int32)\n",
        "      }\n",
        "\n",
        "      predictions = model(inputs)\n",
        "      predicted_class = tf.argmax(predictions, axis=1).numpy()[0]\n",
        "      confidence = tf.reduce_max(predictions).numpy()\n",
        "\n",
        "      result = \"Judi Online\" if predicted_class == 1 else \"Bukan Judi Online\"\n",
        "      return result, confidence\n",
        "\n",
        "def predict_multiple_bert(texts, model, tokenizer):\n",
        "      \"\"\"Batch prediction dengan custom BERT\"\"\"\n",
        "      encoded = tokenizer.encode_batch(texts)\n",
        "      inputs = {\n",
        "            'input_ids': tf.constant(encoded['input_ids'], dtype=tf.int32),\n",
        "            'attention_mask': tf.constant(encoded['attention_mask'], dtype=tf.int32)\n",
        "      }\n",
        "\n",
        "      predictions = model(inputs)\n",
        "      predicted_classes = tf.argmax(predictions, axis=1).numpy()\n",
        "      confidences = tf.reduce_max(predictions, axis=1).numpy()\n",
        "\n",
        "      results = []\n",
        "      for pred, conf in zip(predicted_classes, confidences):\n",
        "            label = \"Judi Online\" if pred == 1 else \"Bukan Judi Online\"\n",
        "            results.append((label, conf))\n",
        "\n",
        "      return results\n",
        "\n",
        "print(\"Prediction functions defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 15: Test Predictions\n",
        "# Test single prediction\n",
        "sample_text = \"ijazah jokowi itu asli penelaah ilmiah itu hanya menebak dan mengira2\"\n",
        "result, confidence = predict_custom_bert(sample_text, model, tokenizer)\n",
        "print(f\"Sample prediction:\")\n",
        "print(f\"Text: '{sample_text}'\")\n",
        "print(f\"Prediction: {result} (confidence: {confidence:.4f})\")\n",
        "\n",
        "# Test multiple predictions\n",
        "sample_texts = [\n",
        "      \"roy suryo itu kan penjahat yang keluar dari penjara\",\n",
        "      \"sehat selalu semuanya salam dari weton88 mudah jackpot\",\n",
        "      \"weton88 tempat paling uhuy\",\n",
        "      \"presiden jokowi memberikan sambutan di acara kemerdekaan\",\n",
        "      \"ayo main slot di situs terpercaya bonus besar\",\n",
        "      \"Bagus sekali podcastnya, isinya bener2 bermanfaat ❤\"\n",
        "]\n",
        "\n",
        "results = predict_multiple_bert(sample_texts, model, tokenizer)\n",
        "print(\"\\nMultiple predictions:\")\n",
        "for text, (result, conf) in zip(sample_texts, results):\n",
        "      print(f\"'{text}' → {result} (confidence: {conf:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # simpan model dan tokenizer ke /models\n",
        "# model.save_pretrained('./fine_tuned_indobert')\n",
        "# tokenizer.save_pretrained('./fine_tuned_indobert')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_combined.head(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model TF-IDF + SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nSkema 1: TF-IDF + SVM (80/20)\")\n",
        "X1 = df_combined['finalText']\n",
        "y1 = df_combined['polarity']\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
        "\n",
        "vectorizer1 = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf1 = vectorizer1.fit_transform(X_train1)\n",
        "X_test_tfidf1 = vectorizer1.transform(X_test1)\n",
        "\n",
        "model1 = SVC(kernel='linear')\n",
        "model1.fit(X_train_tfidf1, y_train1)\n",
        "\n",
        "pred_train1 = model1.predict(X_train_tfidf1)\n",
        "pred_test1 = model1.predict(X_test_tfidf1)\n",
        "print(\"Akurasi Train:\", accuracy_score(y_train1, pred_train1))\n",
        "print(\"Akurasi Test:\", accuracy_score(y_test1, pred_test1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model TF-IDF + MultinomialNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nSkema 1: TF-IDF + SVM (80/20)\")\n",
        "X1 = df_combined['finalText']\n",
        "y1 = df_combined['polarity']\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
        "\n",
        "vectorizer2 = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf1 = vectorizer2.fit_transform(X_train1)\n",
        "X_test_tfidf1 = vectorizer2.transform(X_test1)\n",
        "\n",
        "model2 = MultinomialNB(\n",
        "      alpha=1.0, fit_prior=True, class_prior=None\n",
        ")\n",
        "model2.fit(X_train_tfidf1, y_train1)\n",
        "\n",
        "pred_train1 = model2.predict(X_train_tfidf1)\n",
        "pred_test1 = model2.predict(X_test_tfidf1)\n",
        "print(\"Akurasi Train:\", accuracy_score(y_train1, pred_train1))\n",
        "print(\"Akurasi Test:\", accuracy_score(y_test1, pred_test1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model TF-IDF + LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Skema 3: TF-IDF + Random Forest (70/30)\")\n",
        "X3 = df_combined['text_akhir']\n",
        "y3 = df_combined['polarity']\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, y3, test_size=0.3, random_state=42)\n",
        "\n",
        "vectorizer3 = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf3 = vectorizer3.fit_transform(X_train3)\n",
        "X_test_tfidf3 = vectorizer3.transform(X_test3)\n",
        "\n",
        "model3 = LogisticRegression(n_estimators=100, random_state=42)\n",
        "model3.fit(X_train_tfidf3, y_train3)\n",
        "\n",
        "pred_train3 = model3.predict(X_train_tfidf3)\n",
        "pred_test3 = model3.predict(X_test_tfidf3)\n",
        "print(\"Akurasi Train:\", accuracy_score(y_train3, pred_train3))\n",
        "print(\"Akurasi Test:\", accuracy_score(y_test3, pred_test3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inference_all_models(n=5):\n",
        "      print(\"Tampilkan Inference\\n\")\n",
        "\n",
        "      # Ambil sampel data\n",
        "      sample_data = df_combined.sample(n=n, random_state=52)\n",
        "      sample_cleaned = sample_data['text_akhir'].tolist()\n",
        "      sample_tokens = sample_data['text_tokenizingText'].tolist()\n",
        "      indices = sample_data.index\n",
        "\n",
        "      # TF-IDF Transform untuk model1 dan model3\n",
        "      sample_tfidf1 = vectorizer1.transform(sample_cleaned)\n",
        "      sample_tfidf2 = vectorizer2.transform(sample_cleaned)\n",
        "      sample_tfidf3 = vectorizer3.transform(sample_cleaned)\n",
        "      \n",
        "      # Prediksi dari ketiga model\n",
        "      preds_model1 = model1.predict(sample_tfidf1)\n",
        "      preds_model2 = model2.predict(sample_tfidf2)\n",
        "      preds_model3 = model3.predict(sample_tfidf3)\n",
        "\n",
        "      # Tampilkan hasilnya\n",
        "      for i, idx in enumerate(indices):\n",
        "            original_text = df_combined.loc[idx, 'comment']\n",
        "            print(f\"Teks {i+1} : \\\"{original_text}\\\"\")\n",
        "            print(f\"  Skema 1 (TF-IDF + SVM)          : \\\"{preds_model1[i]}\\\"\")\n",
        "            print(f\"  Skema 2 (Word2Vec + RF)         : \\\"{preds_model2[i]}\\\"\")\n",
        "            print(f\"  Skema 3 (TF-IDF + Random Forest): \\\"{preds_model3[i]}\\\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inference_all_models(n=5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "kBvPl_VJ4heA",
        "rErnbTMGcH9E",
        "JHObJGPW4o_A"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
